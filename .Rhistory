honda
chevrolet
subset(honda, mpg <= 20)
subset(honda, CombMPG <= 20)
nrow(subset(honda, CombMPG <= 20))
nrow(subset(chevrolet, CombMPG <= 20))
hondaMPG <- honda$CombMPG
hondaMPG
chevyMPG <- chevrolet$CombMPG
chevyMPG
boxplot(hondaMPG, chevyMPG, xlab="Honda,Chevrolet")
quartet <- read.table("quartet.csv", sep=",", header=FALSE)
quartet <- read.table("~/engrd2700quartet.csv", sep=",", header=FALSE)
quartet <- read.table("~/engrd2700/quartet.csv", sep=",", header=FALSE)
quartet
summary(quartet$x1)
quartet <- read.table("~/engrd2700/quartet.csv", sep=",", header=TRUE)
quartet
quartet$x1
summary(quartet$x1)
summary(quartet)
par(mfrow=c(2,2))
plot(quartet$x1, quartet$y1, xlab='x1', ylab='y1')
plot(quartet$x1, quartet$y1)
plot(quartet$x1, quartet$y1, xlab='x1', ylab='y1')
plot(quartet$x2, quartet$y2)
quartet <- read.table("~/engrd2700/quartet.csv", sep=",", header=TRUE)
plot(quartet$x1, quartet$y1)
plot(quartet$x1, quartet$y1, xlab="x1", ylab="y1")
plot(quartet$x2, quartet$y2, xlab="x2", ylab="y2")
plot(quartet$x3, quartet$y3, xlab="x3", ylab="y3")
plot(quartet$x4, quartet$y4, xlab="x4", ylab="y4")
compactMG <- read.table("~/engrd2700/compactMPG2015.csv", sep=",", header=TRUE)
compactMG <- read.table("~/engrd2700/compactMPG2015.csv")
compactMG <- data.frame("~/engrd2700/compactMPG2015.csv")
compactMPG2015
compactMPG2015 <- read.csv("~/engrd2700/compactMPG2015.csv"
z
summary(c$CombMPG)
hondaMPG = honda$CombMPG
honda <- subset(compactMPG2015, Make=='Honda')
boxplot(honda$CombMPG, chevrolet$CombMPG, xlab="Honda,Chevrolet")
chevrolet <- subset(c, Make=='Chevrolet')
chevrolet
boxplot(honda$CombMPG, chevrolet$CombMPG, xlab="Honda,Chevrolet")
quantile(honda$CombMPG, 0.25)
quantile(honda$CombMPG, 0.5)
quantile(honda$CombMPG, 0.75)
marathon[, 2]
binom(7, 0.60, 4)
pbinom
pbinom(4, 7, 0.60)
#Kaggle Project Team FA16
#By:Leo Tang
#Additional Tutorial: https://www.kaggle.com/c/titanic/details/new-getting-started-with-r
#Must install packages first using install.packages("insert_package_name")
library(readr)
library(randomForest)
library(caret)
pnorm(0, 1, 0.913)
?pnorm;
pnorm(0.913, 0, 1)
pnorm(1.41, 0, 1)
pnorm(0.92, 0, 1)
pnorm(-1.09, 0, 1)
0.821 - 0.138
pnorm(0.913, 0, 1)
(20-100)/sqrt(100)
(20-25)/sqrt(50./4)
(30 - 25)/sqrt(50./4)
pnorm(1.414, 0, 1)
pnorm(-1.414, 0, 1)
0.92 - 0.07
pnorm(-1, 0, 1)
pbinom(45, 100, .5)
(45.5 - 50)/5
pnorm(-0.9, 0, 1)
pnorm(0.93, 0, 1)
pois(750, 775)
poisson(750)
ppois(750, 775)
pois(775, 750)
ppois(775, 750)
invnorm(0.95)
inverse.gaussian(0.95)
qnorm(0.95, 0, 1)
pnorm(1, 0, 1);
pnorm(-0.9, 0, 1)
ppois(775, 750)
pnorm(0.93, 0, 1)
qnorm(0.95)
1.644*sqrt(12)
5.6949/4.776
pnorm((26.19 - 25)/(4.776/sqrt(12)))
pnorm((27.42 - 25)/(4.776/sqrt(12)))
qnorm(0.95)
1.644/math.sqrt(12)
1.644/sqrt(12)
0.474*4.776
2.263+25
27.42 - 25
_/(5/sqrt(12))
2.42/(5/sqrt(12))
qnorm(1.677)
pnorm(1.677)
1 - 0.953
1.644/sqrt(12)
0.474/5
0.474*5
25+2.37
dt(1.755, 11)
qnorm(-1*sqrt(40)/4) + qnorm(sqrt(40)/4)
pnorm(-1*sqrt(40)/4) + pnorm(sqrt(40)/4)
pnorm(-1*sqrt(40)/4) + (1 - pnorm(sqrt(40)/4))
40*0.5
sqrt(40)*0.5
3.162*2
qnorm(0.025)
-1.9599 + 6.324
sqrt(40)*2
-1.96 + 6.324
4.364/12.6
0.5 - 0.346
0.5 + 0.154
qnorm(-39.93)
pnorm(-39.93)
qnorm(0.05)
-1.64*8
-13.12 + 100
power(95)
power(mu=95)
qnorm(0.01)
qnorm(0.99)
pnorm(0.01)
qnorm(0.01)
qnorm(0.05)
2.33-1.644
0.686^2
8*0.471
3.768/5
# Random forest
xg <- xgboost(data=df_train[features,], nround = 2, objective="reg:linear")
install.packages("xgboost")
library(readr)
library(xgboost)
library(caret)
library(corrplot)
library(gridExtra)
library(ggplot2)
library(lubridate)
train <- read.csv("train.csv", stringsAsFactors=TRUE)
test  <- read.csv("test.csv",  stringsAsFactors=TRUE)
sapply(train, function(x)any(is.na(x)))
#Alley, PoolQC, Fence and MiscFeature have WAY more nulls than the other variables (>1000), so remove them
#---------------------------
train<- train[,-c(7,73,74,75)]
#---------------------------
# Get rid of columns with near zero variance
test <- test[,-c(7,73,74,75)]
nzv <- nearZeroVar(train, saveMetrics= TRUE)
badCols <- nearZeroVar(train)
train_variance <- train[, -badCols]
#---------------------------
test_variance <- test[, -badCols]
#---------------------------
# helper function
extractNumeric <- function(data) {
factor_cols <- names(Filter(function(x) x=="factor", sapply(data, class)))
for (col in factor_cols) {
data[,col] <- ordered(data[,col])
data[,col] <- as.numeric(data[,col])
}
return(data)
}
numerical_train <- extractNumeric(train)
numerical_test <- extractNumeric(test)
setwd("~/datasci/House-Prices")
train <- read.csv("train.csv", stringsAsFactors=TRUE)
test  <- read.csv("test.csv",  stringsAsFactors=TRUE)
sapply(train, function(x)any(is.na(x)))
#Alley, PoolQC, Fence and MiscFeature have WAY more nulls than the other variables (>1000), so remove them
train<- train[,-c(7,73,74,75)]
#---------------------------
test <- test[,-c(7,73,74,75)]
#---------------------------
# Get rid of columns with near zero variance
nzv <- nearZeroVar(train, saveMetrics= TRUE)
badCols <- nearZeroVar(train)
train_variance <- train[, -badCols]
#---------------------------
test_variance <- test[, -badCols]
#---------------------------
# helper function
extractNumeric <- function(data) {
factor_cols <- names(Filter(function(x) x=="factor", sapply(data, class)))
for (col in factor_cols) {
data[,col] <- ordered(data[,col])
data[,col] <- as.numeric(data[,col])
}
return(data)
}
numerical_train <- extractNumeric(train)
numerical_test <- extractNumeric(test)
# delete columns with na values
#numerical_train <- sapply(numerical_train[, colSums(is.na(numerical_train)) > 0], function(col) {
#  col[is.na(col)] <- median(col, na.rm = TRUE)
#});
#str(numerical_train);
#---------------------------
for(i in 1:ncol(numerical_train)){
numerical_train[is.na(numerical_train[,i]), i] <- median(numerical_train[,i], na.rm = TRUE)
}
#---------------------------
#numerical_test <- sapply(numerical_test[, colSums(is.na(numerical_test)) > 0], function(col) {
#  col[is.na(col)] <- median(col, na.rm = TRUE)
#});
#str(numerical_test);
#---------------------------
for(i in 1:ncol(numerical_test)){
numerical_test[is.na(numerical_test[,i]), i] <- median(numerical_test[,i], na.rm = TRUE)
}
#---------------------------
#[is.na(x)] <- median(numerical_train$Fare, na.rm = TRUE)
#nonnan_numerical <- numerical_train[ , colSums(is.na(numerical_train)) == 0]
#M <- cor(nonnan_numerical)
#corrplot(M, tl.cex = .3)
# feature engineering: YrSold and MoSold
numerical_train$MonthAge = (lubridate::year(Sys.Date()) - train$YrSold) * 12 + (lubridate::month(Sys.Date()) - train$MoSold)
numerical_test$MonthAge  = (lubridate::year(Sys.Date()) - test$YrSold)  * 12 + (lubridate::month(Sys.Date()) - test$MoSold)
str(train)
index = createDataPartition(train$Id, p = .8, list = FALSE, times = 1)
df_train = train[index,]
train_y = df_train$count
df_train$Id = NULL
df_test = train[-index,]
test_y = df_test$count
df_test$Id = NULL
features = c("OverallQual", "GrLivArea", "TotalBsmtSF",
"GarageCars", "X2ndFlrSF", "X1stFlrSF", "TotRmsAbvGrd",
"BsmtFinSF1", "LotArea", "MonthAge")
# Random forest
xg <- xgboost(data=df_train[features,], nround = 2, objective="reg:linear")
df_train[features,]
df_train[features]
df_train$SaleCondition
df_train[, features]
df_train[features,]
df_train["OverallQual"]
df_train[c("OverallQual", "GrLivArea")]
df_train[c("OverallQual", "GrLivArea", "TotalBsmtSF")]
df_train[c("OverallQual", "GrLivArea", "TotalBsmtSF", "GarageCars")]
df_train[c("OverallQual", "GrLivArea", "TotalBsmtSF", "GarageCars", "X2ndFlrSF")]
df_train[c("OverallQual", "GrLivArea", "TotalBsmtSF", "GarageCars", "X2ndFlrSF", "TotRmsAbvGrd")]
df_train[c("OverallQual", "GrLivArea", "TotalBsmtSF", "GarageCars", "X2ndFlrSF", "TotRmsAbvGrd", "BsmtFinSF1")]
df_train[c("OverallQual", "GrLivArea", "TotalBsmtSF", "GarageCars", "X2ndFlrSF", "TotRmsAbvGrd", "BsmtFinSF1", "LotArea")]
df_train[c("OverallQual", "GrLivArea", "TotalBsmtSF", "GarageCars", "X2ndFlrSF", "TotRmsAbvGrd", "BsmtFinSF1", "LotArea", "MonthAge")]
source('~/datasci/House-Prices/model_creation_xgboost.R', echo=TRUE)
install.packages("xgboost")
index = createDataPartition(train$Id, p = .8, list = FALSE, times = 1)
df_train = numerical_train[index,]
train_y = df_train$count
df_train$Id = NULL
df_test = numerical_train[-index,]
test_y = df_test$count
df_test$Id = NULL
features = c("OverallQual", "GrLivArea", "TotalBsmtSF",
"GarageCars", "X2ndFlrSF", "X1stFlrSF", "TotRmsAbvGrd",
"BsmtFinSF1", "LotArea", "MonthAge")
# Random forest
xg <- xgboost(data=df_train[features,], nround = 2, objective="reg:linear")
# Random forest
xg <- xgboost(data=df_train[features], nround = 2, objective="reg:linear")
df_train
df_train[features]
xg <- xgboost(data=df_train[features], nround = 2, objective="reg:linear")
head(df_train[features])
xg <- xgboost(data=df_train[features], nround = 2, objective="reg:linear")
?xgboost
xg <- xgboost(data=as.matrix(df_train[features]), nround = 2, objective="reg:linear")
xg <- xgboost(data=as.matrix(df_train[features]), label="SalePrice", nround = 2, objective="reg:linear")
xg <- xgboost(data=as.matrix(df_train[features]), label=features, nround = 2, objective="reg:linear")
xg <- xgboost(data=as.matrix(df_train[features]), label=df_train$Id, nround = 2, objective="reg:linear")
index = createDataPartition(train$Id, p = .8, list = FALSE, times = 1)
df_train = numerical_train[index,]
train_y = df_train$count
df_test = numerical_train[-index,]
test_y = df_test$count
features = c("OverallQual", "GrLivArea", "TotalBsmtSF",
"GarageCars", "X2ndFlrSF", "X1stFlrSF", "TotRmsAbvGrd",
"BsmtFinSF1", "LotArea", "MonthAge")
# Random forest
xg <- xgboost(data=df_train[features], nround = 2, objective="reg:linear")
xg <- xgboost(data=df_train[features], label=df_train$Id, nround = 2, objective="reg:linear")
xg <- xgboost(data=as.matrix(df_train[features]), label=df_train$Id, nround = 2, objective="reg:linear")
xg <- xgboost(data=as.matrix(df_train[features]), label=df_train$Id, nround = 2, objective="reg:linear")
# Predict using the test set (code adapted from public Kaggle script in forums and Leo's example)
prediction <- predict(xg, numerical_test[features,])
# Predict using the test set (code adapted from public Kaggle script in forums and Leo's example)
prediction <- predict(xg, as.matrix(numerical_test[features]))
solution <- data.frame(id = test$Id, SalePrice = prediction)
write.csv(solution, "house_prices_output.csv", row.names = FALSE)
#Change accordingly
source('~/datasci/House-Prices/model_creation.R', echo=TRUE)
install.packages("xgboost")
install.packages("xgboost")
source('~/datasci/House-Prices/model_creation_xgboost.R', echo=TRUE)
install.packages("xgboost")
# Random forest
xg <- xgboost(data=as.matrix(df_train[features]), label=df_train$SalePrice, nround = 2, objective="reg:linear")
train <- read.csv("train.csv", stringsAsFactors=TRUE)
test  <- read.csv("test.csv",  stringsAsFactors=TRUE)
sapply(train, function(x)any(is.na(x)))
#Alley, PoolQC, Fence and MiscFeature have WAY more nulls than the other variables (>1000), so remove them
train<- train[,-c(7,73,74,75)]
#---------------------------
test <- test[,-c(7,73,74,75)]
#---------------------------
# Get rid of columns with near zero variance
nzv <- nearZeroVar(train, saveMetrics= TRUE)
badCols <- nearZeroVar(train)
train_variance <- train[, -badCols]
#---------------------------
test_variance <- test[, -badCols]
#---------------------------
# helper function
extractNumeric <- function(data) {
factor_cols <- names(Filter(function(x) x=="factor", sapply(data, class)))
for (col in factor_cols) {
data[,col] <- ordered(data[,col])
data[,col] <- as.numeric(data[,col])
}
return(data)
}
numerical_train <- extractNumeric(train)
numerical_test <- extractNumeric(test)
# delete columns with na values
#numerical_train <- sapply(numerical_train[, colSums(is.na(numerical_train)) > 0], function(col) {
#  col[is.na(col)] <- median(col, na.rm = TRUE)
#});
#str(numerical_train);
#---------------------------
for(i in 1:ncol(numerical_train)){
numerical_train[is.na(numerical_train[,i]), i] <- median(numerical_train[,i], na.rm = TRUE)
}
#---------------------------
#numerical_test <- sapply(numerical_test[, colSums(is.na(numerical_test)) > 0], function(col) {
#  col[is.na(col)] <- median(col, na.rm = TRUE)
#});
#str(numerical_test);
#---------------------------
for(i in 1:ncol(numerical_test)){
numerical_test[is.na(numerical_test[,i]), i] <- median(numerical_test[,i], na.rm = TRUE)
}
#---------------------------
#[is.na(x)] <- median(numerical_train$Fare, na.rm = TRUE)
#nonnan_numerical <- numerical_train[ , colSums(is.na(numerical_train)) == 0]
#M <- cor(nonnan_numerical)
#corrplot(M, tl.cex = .3)
# feature engineering: YrSold and MoSold
numerical_train$MonthAge = (lubridate::year(Sys.Date()) - train$YrSold) * 12 + (lubridate::month(Sys.Date()) - train$MoSold)
numerical_test$MonthAge  = (lubridate::year(Sys.Date()) - test$YrSold)  * 12 + (lubridate::month(Sys.Date()) - test$MoSold)
str(train)
index = createDataPartition(train$Id, p = .8, list = FALSE, times = 1)
df_train = numerical_train[index,]
train_y = df_train$count
df_test = numerical_train[-index,]
test_y = df_test$count
features = c("OverallQual", "GrLivArea", "TotalBsmtSF",
"GarageCars", "X2ndFlrSF", "X1stFlrSF", "TotRmsAbvGrd",
"BsmtFinSF1", "LotArea", "MonthAge")
# Random forest
xg <- xgboost(data=as.matrix(df_train[features]), label=df_train$SalePrice, nround = 2, objective="reg:linear")
# Predict using the test set (code adapted from public Kaggle script in forums and Leo's example)
prediction <- predict(xg, as.matrix(numerical_test[features]))
solution <- data.frame(id = test$Id, SalePrice = prediction)
write.csv(solution, "house_prices_output.csv", row.names = FALSE)
#function to calculate log error
RMSLE <- function(a, p) {
if (length(a) != length(p)) stop("Actual and Predicted need to be equal lengths!")
x <- !is.na(a)
sqrt(sum((log(p[x]+1) - log(a[x]+1))^2)/sum(x))
}
#Create 5 equally size folds
num_folds = 5
folds <- cut(seq(1,nrow(train)),breaks=num_folds,labels=FALSE)
for(i in 1:num_folds){
#Segement your data by fold using the which() function
testIndexes <- which(folds==i,arr.ind=TRUE)
trainData <- numerical_train[testIndexes, ]
testData <- numerical_train[-testIndexes, ]
test_y = testData$SalePrice
# SalePrice ~ OverallQual + GrLivArea + TotalBsmtSF
# + GarageCars + X2ndFlrSF + X1stFlrSF + TotRmsAbvGrd
#+ BsmtFinSF1 + LotArea + MonthAge + Neighborhood + BsmtQual +
#  HouseStyle + FireplaceQu + GarageFinish + GarageType +
#  CentralAir
xg = xgboost(nround = 2, objective = "reg:linear", data=trainData)
pred = predict(xg, testData)
print(RMSLE(pred, test_y))
}
xg = xgboost(data=as.matrix(trainData), label=trainData$SalePrice, nround = 2, objective="reg:linear")
pred = predict(xg, as.matrix(testData))
print(RMSLE(pred, test_y))
}
install.packages("xgboost")
install.packages("xgboost")
library(readr)
library(xgboost)
library(caret)
library(corrplot)
library(gridExtra)
library(ggplot2)
library(lubridate)
train <- read.csv("train.csv", stringsAsFactors=TRUE)
test  <- read.csv("test.csv",  stringsAsFactors=TRUE)
sapply(train, function(x)any(is.na(x)))
#Alley, PoolQC, Fence and MiscFeature have WAY more nulls than the other variables (>1000), so remove them
train<- train[,-c(7,73,74,75)]
#---------------------------
#---------------------------
nzv <- nearZeroVar(train, saveMetrics= TRUE)
train_variance <- train[, -badCols]
test_variance <- test[, -badCols]
# helper function
factor_cols <- names(Filter(function(x) x=="factor", sapply(data, class)))
test <- test[,-c(7,73,74,75)]
data[,col] <- ordered(data[,col])
badCols <- nearZeroVar(train)
#---------------------------
#---------------------------
return(data)
}
data[,col] <- as.numeric(data[,col])
numerical_test <- extractNumeric(test)
# delete columns with na values
#numerical_train <- sapply(numerical_train[, colSums(is.na(numerical_train)) > 0], function(col) {
#  col[is.na(col)] <- median(col, na.rm = TRUE)
}
for (col in factor_cols) {
numerical_train <- extractNumeric(train)
# Get rid of columns with near zero variance
extractNumeric <- function(data) {
#});
#str(numerical_train);
#---------------------------
for(i in 1:ncol(numerical_train)){
numerical_train[is.na(numerical_train[,i]), i] <- median(numerical_train[,i], na.rm = TRUE)
}
#---------------------------
#numerical_test <- sapply(numerical_test[, colSums(is.na(numerical_test)) > 0], function(col) {
#  col[is.na(col)] <- median(col, na.rm = TRUE)
#});
#str(numerical_test);
#---------------------------
for(i in 1:ncol(numerical_test)){
numerical_test[is.na(numerical_test[,i]), i] <- median(numerical_test[,i], na.rm = TRUE)
}
#---------------------------
#[is.na(x)] <- median(numerical_train$Fare, na.rm = TRUE)
#nonnan_numerical <- numerical_train[ , colSums(is.na(numerical_train)) == 0]
#M <- cor(nonnan_numerical)
#corrplot(M, tl.cex = .3)
# feature engineering: YrSold and MoSold
numerical_train$MonthAge = (lubridate::year(Sys.Date()) - train$YrSold) * 12 + (lubridate::month(Sys.Date()) - train$MoSold)
numerical_test$MonthAge  = (lubridate::year(Sys.Date()) - test$YrSold)  * 12 + (lubridate::month(Sys.Date()) - test$MoSold)
str(train)
index = createDataPartition(train$Id, p = .8, list = FALSE, times = 1)
df_train = numerical_train[index,]
train_y = df_train$count
df_test = numerical_train[-index,]
test_y = df_test$count
features = c("OverallQual", "GrLivArea", "TotalBsmtSF",
"GarageCars", "X2ndFlrSF", "X1stFlrSF", "TotRmsAbvGrd",
"BsmtFinSF1", "LotArea", "MonthAge")
# Random forest
xg <- xgboost(data=as.matrix(df_train[features]), label=df_train$SalePrice, nround = 2, objective="reg:linear")
# Predict using the test set (code adapted from public Kaggle script in forums and Leo's example)
prediction <- predict(xg, as.matrix(numerical_test[features]))
solution <- data.frame(id = test$Id, SalePrice = prediction)
write.csv(solution, "house_prices_output.csv", row.names = FALSE)
#function to calculate log error
RMSLE <- function(a, p) {
if (length(a) != length(p)) stop("Actual and Predicted need to be equal lengths!")
x <- !is.na(a)
sqrt(sum((log(p[x]+1) - log(a[x]+1))^2)/sum(x))
}
#Create 5 equally size folds
num_folds = 5
folds <- cut(seq(1,nrow(train)),breaks=num_folds,labels=FALSE)
for(i in 1:num_folds){
#Segement your data by fold using the which() function
testIndexes <- which(folds==i,arr.ind=TRUE)
trainData <- numerical_train[testIndexes, ]
testData <- numerical_train[-testIndexes, ]
test_y = testData$SalePrice
# SalePrice ~ OverallQual + GrLivArea + TotalBsmtSF
# + GarageCars + X2ndFlrSF + X1stFlrSF + TotRmsAbvGrd
#+ BsmtFinSF1 + LotArea + MonthAge + Neighborhood + BsmtQual +
#  HouseStyle + FireplaceQu + GarageFinish + GarageType +
#  CentralAir
xg = xgboost(data=as.matrix(trainData), label=trainData$SalePrice, nround = 2, objective="reg:linear")
pred = predict(xg, as.matrix(testData))
print(RMSLE(pred, test_y))
}
source('~/datasci/House-Prices/model_creation_xgboost.R', echo=TRUE)
#+ BsmtFinSF1 + LotArea + MonthAge + Neighborhood + BsmtQual +
source('~/datasci/House-Prices/model_creation_xgboost.R', echo=TRUE)
source('~/datasci/House-Prices/model_creation_xgboost.R', echo=TRUE)
source('~/datasci/House-Prices/model_creation_xgboost.R', echo=TRUE)
source('~/datasci/House-Prices/model_creation_xgboost.R', echo=TRUE)
source('~/datasci/House-Prices/model_creation_xgboost.R', echo=TRUE)
source('~/datasci/House-Prices/model_creation_xgboost.R', echo=TRUE)
